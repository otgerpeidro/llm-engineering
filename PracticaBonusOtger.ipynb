{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN4Q1xIRSfe8"
      },
      "source": [
        "### **Paso 1: Configurar Google Colab para Streamlit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkrdpydWRtnZ",
        "outputId": "4cb6ebca-29ad-4bd9-ae78-2d39f38773df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU streamlit openai pyngrok\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3AQDvGpSk01"
      },
      "source": [
        "### **Paso 2: Código para cada aplicación de chat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mF-GsNGvSP4X"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJsQWhqWSrTW"
      },
      "source": [
        "# 1. Echo Bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc3gFTNfR83A",
        "outputId": "54da0d00-3074-466d-badf-6ddeb4c15bd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-22 09:35:31.899 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "st.title(\"Echo Bot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVZ2AnopTCeS",
        "outputId": "58e25d79-1f51-4ee6-833e-b9533c68492e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-22 09:35:44.355 Session state does not function when running a script without `streamlit run`\n"
          ]
        }
      ],
      "source": [
        "# Inicializar el historial de chat\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fA07xpSATExa"
      },
      "outputs": [],
      "source": [
        "# Mostrar mensajes del historial en la recarga de la app\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bbGpMfxvTGqA"
      },
      "outputs": [],
      "source": [
        "# Reaccionar a la entrada del usuario\n",
        "if prompt := st.chat_input(\"What is up?\"):\n",
        "    # Mostrar el mensaje del usuario en el contenedor de mensajes del chat\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "    # Añadir el mensaje del usuario al historial de chat\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    response = f\"Echo: {prompt}\"\n",
        "    # Mostrar la respuesta del asistente en el contenedor de mensajes del chat\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    # Añadir la respuesta del asistente al historial de chat\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4KvYGcDS0CK"
      },
      "source": [
        "# 2. Simple Chatbot GUI con streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J0qA3tf7TXnp"
      },
      "outputs": [],
      "source": [
        "# Emulador de respuesta en streaming\n",
        "def response_generator():\n",
        "    response = random.choice(\n",
        "        [\n",
        "            \"Hello there! How can I assist you today?\",\n",
        "            \"Hi, human! Is there anything I can help you with?\",\n",
        "            \"Do you need help?\",\n",
        "        ]\n",
        "    )\n",
        "    for word in response.split():\n",
        "        yield word + \" \"\n",
        "        time.sleep(0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjeoV29QTZwR",
        "outputId": "49d426d2-c502-4309-e4db-be2a699f8371"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "st.title(\"Simple chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "k-bR5AaxTdHY"
      },
      "outputs": [],
      "source": [
        "# Inicializar el historial de chat\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V4TGG9PDTfnT"
      },
      "outputs": [],
      "source": [
        "# Mostrar mensajes del historial en la recarga de la app\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UVkkRWu-R-TJ"
      },
      "outputs": [],
      "source": [
        "# Aceptar la entrada del usuario\n",
        "if prompt := st.chat_input(\"What is up?\"):\n",
        "    # Añadir el mensaje del usuario al historial de chat\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    # Mostrar el mensaje del usuario en el contenedor de mensajes del chat\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Mostrar la respuesta del asistente en el contenedor de mensajes del chat\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        response = st.write_stream(response_generator())\n",
        "    # Añadir la respuesta del asistente al historial de chat\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-nZeDBQS5WN"
      },
      "source": [
        "# 3. Aplicación similar a ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYYwPq98TovD",
        "outputId": "2fdfd573-0d2d-4fe4-bbfd-f6483a6deebb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "st.title(\"ChatGPT-like clone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HRV5zlzOUY4z"
      },
      "outputs": [],
      "source": [
        "# Configuramos la API key de OpenAI\n",
        "openai_api_key = \"YOUR_API_KEY\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "66_U0tbvSAsh"
      },
      "outputs": [],
      "source": [
        "if \"openai_model\" not in st.session_state:\n",
        "    st.session_state[\"openai_model\"] = \"gpt-3.5-turbo\"\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gzFft-r3VztR"
      },
      "outputs": [],
      "source": [
        "# Mostrar mensajes del historial en la recarga de la app\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qZWP4lpNVv2b"
      },
      "outputs": [],
      "source": [
        "# Aceptar la entrada del usuario\n",
        "if prompt := st.chat_input(\"What is up?\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        # Llamada a la API de OpenAI para generar una respuesta\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=st.session_state[\"openai_model\"],\n",
        "            messages=[\n",
        "                {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n",
        "                for m in st.session_state.messages\n",
        "            ],\n",
        "            stream=True,\n",
        "        )\n",
        "        # Mostrar la respuesta del asistente en el contenedor de mensajes del chat\n",
        "        response_content = \"\".join([chunk['choices'][0]['delta'].get('content', '') for chunk in response])\n",
        "        st.markdown(response_content)\n",
        "\n",
        "    # Añadir la respuesta del asistente al historial de chat\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_content})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01dHaMCoS9_2"
      },
      "source": [
        "### **Paso 3: Ejecutar Streamlit en Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VY1Wqla6XOpn"
      },
      "outputs": [],
      "source": [
        "# Especificar tu authtoken de ngrok aquí\n",
        "ngrok_authtoken = \"YOUR_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-xC88xK2XURt"
      },
      "outputs": [],
      "source": [
        "# Autenticar ngrok con tu authtoken\n",
        "ngrok.set_auth_token(ngrok_authtoken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kOZiTCEoSFHV"
      },
      "outputs": [],
      "source": [
        "# Crear un archivo para la aplicación Streamlit (por ejemplo, app.py)\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Echo Bot\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"What is up?\"):\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    response = f\"Echo: {prompt}\"\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jGqxgiNaV6Wu"
      },
      "outputs": [],
      "source": [
        "# Ejecutar la aplicación Streamlit en segundo plano\n",
        "process = subprocess.Popen(['streamlit', 'run', 'app.py'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4DbI6DQV8IR",
        "outputId": "ff2b0452-1d84-4ae4-eb20-2073e4b94a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://3473-34-16-179-107.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "# Crear un túnel a través de ngrok\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(f'Public URL: {public_url}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "2kEQL3gdV9uD",
        "outputId": "31621e9b-79b0-4ba1-b09b-4439d1dc0e52"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-05e1fc3390a5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mantener el proceso de Streamlit en ejecución\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1960\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Mantener el proceso de Streamlit en ejecución\n",
        "process.wait()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
